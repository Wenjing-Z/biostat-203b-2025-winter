---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: "Wenjing Zhou and 806542441"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

### 1. Data preprocessing and feature engineering.

#### Loading necessary libraries

```{r}
library(dplyr)
library(tidymodels)
library(vip)  # For variable importance
library(xgboost)
library(doParallel) # For parallel processing
library(stacks)
library(pROC)
```
#### Loading dataset
```{r}
icu_data <- readRDS("mimic_icu_cohort.rds") %>%
    mutate(los_long = los > 2)
icu_data
```


#### Data cleaning

```{r}
icu_clean <- icu_data %>%
  select(-subject_id, -hadm_id, -stay_id, -last_careunit, -intime, -outtime, 
         -dischtime,-admittime, -admit_provider_id, -deathtime, -edregtime, 
         -edouttime, -dod) %>%
  select(-c(discharge_location, hospital_expire_flag, los)) %>%
  select(-c(anchor_year, anchor_year_group)) %>% # Remove future or unnecessary info
  mutate(
    los_long = factor(los_long, levels = c(FALSE, TRUE), labels = c("Yes", "No")),
    gender = factor(gender),
    marital_status = factor(marital_status),
    race = factor(race),
    first_careunit = factor(first_careunit)
  ) %>%
  drop_na()
```

### 2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.

#### Train-Test Split

```{r}
#Train-Test Split
set.seed(203)

data_split <- initial_split(
  icu_clean, 
  # stratify by los_long                          
  prop = 0.5, 
  strata = los_long
  )

train_data <- training(data_split)
test_data  <- testing(data_split)
```

#### Cross-validation folds
```{r}
set.seed(203)

folds <- vfold_cv(train_data, v = 5, strata = los_long)

icu_folds <- folds
```

#### Recipe
```{r}
icu_recipe <- recipe(los_long ~ ., data = train_data) %>%
  step_novel(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.01) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%    # 新增这一步去掉无用列
  step_normalize(all_numeric_predictors())  # Standardize numeric features
```

### 3. Train and tune the models using the training set.

#### Model 1: Logistic Regression (Elastic Net)

```{r}
log_reg_spec <- logistic_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_engine("glmnet")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(icu_recipe)

set.seed(123)

log_reg_grid_fine <- grid_regular(
  penalty(range =10^c(-6, -1)),  
  mixture(range = c(0, 1)),  
  levels = 5
)

log_reg_res_fine <- tune_grid(
  log_reg_workflow,
  resamples = vfold_cv(train_data, v = 5, strata = los_long),
  grid = log_reg_grid_fine,
  metrics = metric_set(roc_auc)
)

# show 25 models
log_reg_res_fine %>% collect_metrics()
```
```{r}
# Show the best roc
log_reg_res_fine %>% show_best(metric = "roc_auc")
```

```{r}
# choose the best model
best_fine <- log_reg_res_fine %>% 
  select_best(metric = "roc_auc")

best_fine
```

Make sure the final model is trained on the entire training set and evaluated on the test set.
```{r}
final_workflow_fine <- log_reg_workflow %>% 
  finalize_workflow(best_fine)

# train the final model on the full training set
final_fit_fine <- final_workflow_fine %>% 
  last_fit(split = data_split)

# evaluat on the test set
final_fit_fine %>% collect_metrics()
```
#### Model 2: Random Forest

```{r}
set.seed(203)

# define the RF model
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500
) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# set workflow
rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(icu_recipe)

# grid search
rf_grid <- grid_random(
  mtry(range = c(2, 10)),
  min_n(range = c(5, 20)),
  size = 10
)

# tune grid
rf_res <- tune_grid(
  rf_workflow,
  resamples = folds,
  grid = rf_grid,
  metrics = metric_set(roc_auc)
)
```
```{r}
rf_res %>% show_best(metric ="roc_auc")
```

```{r}
# choose the best parameters
best_rf <- rf_res %>% select_best(metric = "roc_auc")
best_rf

# final workflow
final_rf_workflow <- rf_workflow %>% finalize_workflow(best_rf)

# fit the final model
final_rf_fit <- final_rf_workflow %>% last_fit(data_split)

# evaluate on the test set
final_rf_fit %>% collect_metrics()

```

Importance of features (vip)
```{r}
# fit the final model to whole training set
final_rf_fit <- final_rf_workflow %>%
  last_fit(data_split)

# feature importance
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15)
```



#### Model 3: XGBoost

```{r}
xgb_spec <- boost_tree(
  trees = 500,
  tree_depth = tune(),
  learn_rate = tune(),
  min_n = tune(),
  #loss_reduction = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```


```{r}
xgb_workflow <- workflow() %>%
  add_recipe(icu_recipe) %>%
  add_model(xgb_spec)

```

```{r}
set.seed(123)

xgb_grid <- grid_random(
  learn_rate(range = c(-3, -1)),   
  tree_depth(range = c(3, 8)),     
  min_n(range = c(5, 20)),          
  size = 10                       
)
```


Perform cross-validation for hyperparameter tuning (enable parallel acceleration)

```{r}
library(doFuture)
library(doParallel)

cores <- parallel::detectCores() - 1
registerDoFuture()
plan(multisession, workers = cores)

set.seed(123)
xgb_res <- tune_grid(
  xgb_workflow,
  resamples = vfold_cv(train_data, v = 5, strata = los_long),
  grid = xgb_grid,
  metrics = metric_set(roc_auc)
)

plan(sequential)

```


```{r}
xgb_res %>% show_best(metric ="roc_auc")
```

Review tuning results and select the best model.


```{r}
# review tuning results
xgb_res %>% show_best( metric = "roc_auc")

# select the best model
best_xgb <- xgb_res %>% select_best( metric = "roc_auc")
best_xgb
```

Evaluate the results by best parameters.
```{r}
# 最终workflow确定
final_xgb_workflow <- xgb_workflow %>% finalize_workflow(best_xgb)

# 训练最终模型并在测试集评估
final_xgb_fit <- final_xgb_workflow %>% last_fit(data_split)

# 测试集表现评估
final_xgb_fit %>% collect_metrics()

```

Feature Importance Analysis for the Model
```{r}

final_xgb_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15)

```

### 4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?

#### Model Performance Evaluation on the Test Set 

Evaluating and Comparing Model Performance

```{r}
# Extract the predictions of each model on the test set
lr_preds <- collect_predictions(final_fit_fine)
rf_preds <- collect_predictions(final_rf_fit)
xgb_preds <- collect_predictions(final_xgb_fit)

# calculate ROC AUC & Accuracy
model_metrics <- bind_rows(
  lr_preds %>% mutate(model = "Logistic Regression"),
  rf_preds %>% mutate(model = "Random Forest"),
  xgb_preds %>% mutate(model = "XGBoost")
) %>%
  group_by(model) %>%
  summarise(
    accuracy = accuracy_vec(truth = los_long, estimate = .pred_class),
    roc_auc = roc_auc_vec(truth = los_long, estimate = .pred_Yes)
  )

# show the results
model_metrics
```

```{r}
# Define all final workflow parameters
final_rf_workflow <- rf_workflow %>% finalize_workflow(best_rf)
final_xgb_workflow <- xgb_workflow %>% finalize_workflow(best_xgb)
final_log_workflow <- log_reg_workflow %>% finalize_workflow(best_fine)

# run the final models
final_rf_fit <- final_rf_workflow %>% last_fit(data_split)
final_xgb_fit <- final_xgb_workflow %>% last_fit(data_split)
final_log_fit <- final_log_workflow %>% last_fit(data_split)

# Perform the standard evaluation of the results
final_rf_fit %>% collect_metrics()
final_xgb_fit %>% collect_metrics()
final_log_fit %>% collect_metrics()
```

#### Model Stacking Code 

```{r}
set.seed(123)

# define the stacking model
log_reg_res <- final_log_workflow %>% 
  fit_resamples(
    resamples = folds, 
    control = control_stack_resamples()
  )

rf_res <- final_rf_workflow %>% 
  fit_resamples(
    resamples = folds,  
    control = control_stack_resamples()
  )

xgb_res <- final_xgb_workflow %>% 
  fit_resamples(
    resamples = folds, 
    control = control_stack_resamples()
  )
```


```{r}
# Develop stacking models
model_stack <- stacks() %>%
  add_candidates(log_reg_res) %>%
  add_candidates(rf_res) %>%
  add_candidates(xgb_res) %>%
  blend_predictions() %>%
  fit_members()

stack_preds <- predict(model_stack, test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  mutate(.pred_class = factor(ifelse(.pred_Yes > 0.5, "Yes", "No"),
                              levels = c("Yes", "No")))

roc_auc(stack_preds, truth = los_long, .pred_Yes)
accuracy(stack_preds, truth = los_long, .pred_class)

```

Model Performance Evaluation (ROC & Accuracy)
```{r}
# Extract Model Performance
log_metrics <- final_log_fit %>% collect_metrics() %>% select(.metric, .estimate)
rf_metrics  <- final_rf_fit  %>% collect_metrics() %>% select(.metric, .estimate)
xgb_metrics <- final_xgb_fit %>% collect_metrics() %>% select(.metric, .estimate)

# Stacking Model Performance
stack_roc <- roc_auc(stack_preds, truth = los_long, .pred_Yes)
stack_acc <- accuracy(stack_preds, truth = los_long, .pred_class)

# Show Combined Results
results <- tibble(
  Model = c("Logistic Regression", "Random Forest", "XGBoost", "Stacking"),
  ROC_AUC = c(log_metrics$.estimate[log_metrics$.metric=="roc_auc"],
              rf_metrics$.estimate[rf_metrics$.metric=="roc_auc"],
              xgb_metrics$.estimate[xgb_metrics$.metric=="roc_auc"],
              stack_roc$.estimate),
  Accuracy = c(log_metrics$.estimate[log_metrics$.metric=="accuracy"],
               rf_metrics$.estimate[rf_metrics$.metric=="accuracy"],
               xgb_metrics$.estimate[xgb_metrics$.metric=="accuracy"],
               stack_acc$.estimate)
)

results
```

Model Feature Importance Analysis
```{r}
# Random Forest Feature Importance
final_rf_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15) +
  ggtitle("Random Forest Feature Importance")

# XGBoost Feature Importance
final_xgb_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 15) +
  ggtitle("XGBoost Feature Importance")

```

Model Performance Comparison (ROC Curve Plot)

```{r}
# Extract Prediction Probabilities
log_preds <- final_log_fit %>% collect_predictions()
rf_preds  <- final_rf_fit %>% collect_predictions()
xgb_preds <- final_xgb_fit %>% collect_predictions()

# ROC Curve
log_roc <- roc(log_preds$los_long, log_preds$.pred_Yes)
rf_roc  <- roc(rf_preds$los_long, rf_preds$.pred_Yes)
xgb_roc <- roc(xgb_preds$los_long, xgb_preds$.pred_Yes)
stack_roc_curve <- roc(stack_preds$los_long, stack_preds$.pred_Yes)

plot(log_roc, col = "blue", legacy.axes=TRUE, main="ROC Curve Comparison")
plot(rf_roc, col = "green", add = TRUE)
plot(xgb_roc, col = "red", add = TRUE)
plot(stack_roc_curve, col = "purple", add = TRUE)

legend("bottomright",
       legend=c("Logistic Regression","Random Forest","XGBoost","Stacking"),
       col=c("blue","green","red","purple"), lwd=2)

```

| Mode                | Roc_Auc     | Accuracy |
|---------------------|-------------|----------|
| Logistic Regression | $0.5958$    | 0.5576   |
| Random Forest       | $0.6489$    | 0.6062   |
| XGBoost             | $0.6488$    | 0.6099   |
| Stacking            | $0.6538$    | 0.6107   |


The stacking model slightly outperforms the individual models in terms of ROC-AUC (0.6538), followed closely by XGBoost (0.6488) and Random Forest (0.6489). Logistic regression has significantly lower performance (0.5958), indicating its limited predictive capability for this dataset.

Similar trends emerge in accuracy, with stacking achieving the highest accuracy (0.6107). However, accuracy differences between stacking, XGBoost, and Random Forest are relatively small, indicating that the stacking method's improvement, although statistically measurable, is practically modest.
